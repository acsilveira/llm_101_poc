LLM 101 PoC, todo
[DONE]  Setup environment
[DONE]      Build docker image
[DONE]          Set env vars file
[DONE]  Try LangChain asking ChatGPT
[DONE]          Understand if possible use OpenAI API for free
[DONE]  Ask a question about an web article
[DONE]  Ask a question about an PDF file
[DONE]  Build an simple app with streamlit
[DONE]          Minimal app running in streamlit
[DONE]          Build form with: text edit for question and file uploader button
[DONE]  Make llm chain work totally by code, without human interaction
[DONE]  Insert llm pipeline in the streamlit interface
[DONE]  Improve front-end
[DONE]          Upload PDF before questions input
[DONE]  Improve LLM answer
[DONE]          Use documents split by header, not splitted 2 times
[DONE]          Find the point to wait for the vectorstore be ready
[DONE]          Allow ask again using vectors already loaded
[DONE]                  Create methods for calss controller_llm
[DONE]          Display log step by step to the user
[DONE]                  Use response of each method to print log in streamlit
[DONE]          Learn how to break text in chunks for a better LLM answer
[DONE]  Deploy the app to the web
[DONE]      AWS and GCP image recap
[DONE]          Check pricing
[DONE]      Try deploy for free
[DONE]          Build a lean docker image trying to stay below 0.5 GB
[DONE]              Fix version of the app current version
[DONE]          If for free, deploy v0v1 version to web
[DONE]              Deploy in GCP respecting  always free limits
[DONE]          Avoid shutdown app if the GCP web console get closed
[DONE]  Improve answer
[DONE]      from PDFToChat
[DONE]          Check chunks strategy
[DONE]          Check LLM and embedding strategy
[DONE]              Implement mode OpenAI as LLM
[DONE]  Logging
[DONE]      Implement real logging
        Tech debt
            Write unit tests
[wip]           for utils
                for controller
                for app
            Check availability of vectorstore instead of wait