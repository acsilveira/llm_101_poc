LLM 101 PoC, ToDo
[DONE]  Setup environment
[DONE]      Build docker image
[DONE]          Set env vars file
[DONE]  Try LangChain asking ChatGPT
[DONE]          Understand if possible use OpenAI API for free
[DONE]  Ask a question about an web article
[DONE]  Ask a question about an PDF file
[DONE]  Build an simple app with streamlit
[DONE]          Minimal app running in streamlit
[DONE]          Build form with: text edit for question and file uploader button
[DONE]  Make llm chain work totally by code, without human interaction
[DONE]  Insert llm pipeline in the streamlit interface
[DONE]  Improve front-end
[DONE]          Upload PDF before questions input
[DONE]  Improve LLM answer
[DONE]          Use documents split by header, not splitted 2 times
[DONE]          Find the point to wait for the vectorstore be ready
[DONE]          Allow ask again using vectors already loaded
[DONE]                  Create methods for calss controller_llm
[DONE]          Display log step by step to the user
[DONE]                  Use response of each method to print log in streamlit
[DONE]          Learn how to break text in chunks for a better LLM answer
[DONE]  Deploy the app to the web
[DONE]      AWS and GCP image recap
[DONE]          Check pricing
[DONE]      Try deploy for free
[DONE]          Build a lean docker image trying to stay below 0.5 GB
[DONE]              Fix version of the app current version
[DONE]          If for free, deploy v0v1 version to web
[DONE]              Deploy in GCP respecting  always free limits
[DONE]          Avoid shutdown app if the GCP web console get closed
[DONE]  Improve answer
[DONE]      from PDFToChat
[DONE]          Check chunks strategy
[DONE]          Check LLM and embedding strategy
[DONE]              Implement mode OpenAI as LLM
[DONE]  Logging
[DONE]      Implement real logging
[DONE]  Implement button submit to URL input
[DONE]  Keep the URL saved to avoid upload the same content unnecessarily
[DONE]      Fix deletion
[DONE]      Check and reorg code
[DONE]  Re-allow content by PDF upload
[DONE]      Front-end
[DONE]      Back-end
[DONE]  Show in the UI: the model choice, URL
[DONE]      Change model choice to radio element
[DONE]      Create another choice for "all text" and "text filtered by relevance"
[DONE]      Show the choices for specific the screens
[DONE]  Allow UI change the model choice for further questions
[DONE]  Allow question inputting all(limited) content
[DONE]  Find a way to show logging in the UI
[DONE]      Re-implement logging to show to the user each step happening: https://discuss.streamlit.io/t/redirecting-logger-output-to-a-streamlit-widget/61070/2
[DONE]      Solve the error message popup "Bad 'setIn'"
[DONE]      Show option choice for content type PDF|URL only in the appropriated states
[DONE]      If the option choice for content type PDF|URL changed then the chain should be rebuilt
        ---v0.2
        Review notebooks
        Review python files
        Review README
        Review guide
        Review license
        ---open source
        Tech debt
[wip]       Reorg code as best practice
[wip]           SOLID
[DONE]              Understand SOLID
[wip]               Identify improvements to respect SOLID principles
[wip]           Model, View, Controller
[DONE]              Move workflow to controller
[wip]           Ask improvement suggestions to chatGPT or Gemini
[wip]               Act on chatGPT suggestions: https://chatgpt.com/c/20a12a4d-c0ad-4c43-82f7-46c32e43f445
            Write unit tests
[wip]           for utils
                for controller
                for app
            Check availability of vector store instead of wait
            Create a model class to the vector store